<pattern-file>
  <meta>
    <pattern-id>validate-before-implement-pattern</pattern-id>
    <source>doc/claude-test-brain.xml</source>
    <version>1.3</version>
  </meta>
  <pattern>
      <name>Validate Existing Implementation Before Work</name>
      <when>Starting a new feature implementation or enhancement</when>
      <philosophy>Check what exists first - avoid implementing what's already done</philosophy>
      <complete-example>
        <file>Real-world example: Phase 1 Error Reporting</file>
        <code>
# WRONG APPROACH: Start implementing based on TODO without validation
# Result: Waste 6-8 hours re-implementing what already exists

# RIGHT APPROACH: Validate first
# Step 1: Read related files to understand current state
@Read("src/ecreshore/services/error_handler.py")
@Read("src/ecreshore/services/batch_error_aggregator.py")
@Read("src/ecreshore/services/batch_progress.py")

# Step 2: Search for integration points
@Grep(pattern="error_aggregator", output_mode="files_with_matches")
@Grep(pattern="add_transfer_error", output_mode="content")

# Step 3: Create test to validate current behavior
# Create test-error-reporting.yaml with intentional failures
transfers:
  - source: ghcr.io/nonexistent/fake-image
    target: test-fake
  - source: docker.io/library/alpine:3.18
    target: test-missing-repo

# Step 4: Run test to observe current error reporting
$ uv run ecreshore batch test-error-reporting.yaml --simple
$ uv run ecreshore batch test-error-reporting.yaml --rich

# Step 5: Analyze results
# Discovery: BatchErrorAggregator already exists (300 lines, production-ready)
# Discovery: Error categorization fully implemented (12 categories)
# Discovery: Error summary UI complete for simple + rich modes
# Discovery: Only missing: verbose log cleanup (6 lines to change)

# Result: 6-8 hour estimate &amp;#8594; 1 hour actual (95% already done)
</code>
      </complete-example>
      <critical-rules>
        <rule>ALWAYS read related implementation files before estimating</rule>
        <rule>Search for integration points with Grep</rule>
        <rule>Create test cases to validate current behavior</rule>
        <rule>Document what exists vs. what's missing</rule>
        <rule>Revise effort estimates based on findings</rule>
      </critical-rules>
      <validation-checklist>
        <step>1. Read files mentioned in TODO/requirements</step>
        <step>2. Search for related patterns (Grep for keywords)</step>
        <step>3. Check for existing tests of similar functionality</step>
        <step>4. Create minimal test to validate behavior</step>
        <step>5. Document architectural findings</step>
        <step>6. Update effort estimate based on reality</step>
      </validation-checklist>
      <real-world-impact>
        <case>
          <task>Phase 1: Batch Error Output Enhancement</task>
          <original-estimate>6-8 hours implementation</original-estimate>
          <after-validation>1 hour (95% already complete)</after-validation>
          <discovery>BatchErrorAggregator, ErrorCategorizer, error summary UI all production-ready</discovery>
          <actual-work>Changed 6 lines (logger.error â†’ logger.debug)</actual-work>
        </case>
      </real-world-impact>
    </pattern>
</pattern-file>
